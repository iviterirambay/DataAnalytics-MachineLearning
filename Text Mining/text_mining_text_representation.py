# -*- coding: utf-8 -*-
"""Text Mining - Text Representation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QWabOeIPlsDsyp7Dvx9nDPt7_nBJk27L
"""

import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics.pairwise import euclidean_distances
from nltk.corpus import stopwords

class TfIdfModel(object):

    def __init__(self,corpus,vocabulary=None):
        super(TfIdfModel, self).__init__()
        self.corpus = corpus
        self.vocabulary = vocabulary
        self.matrix = None

    def fit(self):
        if self.vocabulary:
            self.model = TfidfVectorizer(min_df=1, vocabulary=self.vocabulary, norm = 'l2')
        else:
            self.model = TfidfVectorizer(analyzer='word',ngram_range=(1,2))
        self.document_term = self.model.fit_transform(self.corpus).toarray()
        self.features = self.model.get_feature_names()
        self.idf = self.model.idf_

    def project(self,query_text):
        query_vector = self.model.transform([query_text]).toarray()[0]
        return query_vector

    def get_nearest_neighbors(self,vector_query,n_top,similarity_metrics):
        similarities = similarity_metrics(self.document_term,np.array([vector_query]))#pairwise similarity: calcula la similaridad entre la query y el conjunto de documentos
        scores = [ (doc_index,score[0]) for doc_index,score in enumerate(similarities)]#se incorpora el id del documento
        scores = sorted(scores, key=lambda tup: tup[1],reverse=True)		
        return scores[:n_top]#se seleccionan los n_top elementos mas similares

    def retrieve_docs(self,query_text,n_top=10,similarity_metrics=cosine_similarity):
        vector_query = self.project(query_text)
        nearest_neighbors = self.get_nearest_neighbors(vector_query,n_top,similarity_metrics)
        #nearest_neighbors es una lista (doc_index,score) con los documentos mas similares a la query
        docs_index = [doc_index for doc_index,score in nearest_neighbors]
        docs = []
        for key in docs_index:
            docs.append(self.corpus[key])
        return docs

import os
import json
import re
import unicodedata
import sys
from nltk.corpus import stopwords

class FileStorage():

    def __init__(self,filename):
        self.filename = filename

    def read(self):
        if os.path.exists(self.filename):
            with open(self.filename) as file:
                data = json.load(file)
                return data
        else:
            return {}
        
    def save(self,data):#write data like json file
        try:
            old_data = self.read()
            if len(old_data.keys()) == 0:
                old_data["tweets"] = []
            old_data["tweets"].append(data)
            jsondata = json.dumps(old_data, indent=4, skipkeys=True, sort_keys=True)
            fd = open(self.filename, 'w')
            fd.write(jsondata)
            fd.close()
            print (self.filename + " ha sido escrito exitosamente")
        except Exception as e:
            print (e)
            print ('ERROR writing', self.filename)

#remove stop words
import nltk
from nltk.corpus import stopwords
import os
import json
import re
import sys
import unicodedata


def filter_stop_words(text):
    stop_words_list = stopwords.words('spanish')
    stop_words_list += stopwords.words('english')
    text_filtered = [word for word in text.split() if word.lower() not in stop_words_list]
    return text_filtered

def process(tweet):
    tweet = tweet.lower()
    tweet = re.sub(r"(https?)\S+","",tweet)#remove urls
    #tweet = re.sub(r"(\B#)\w*","",tweet)#remove hashtags
    tweet = re.sub(r"(\B@)\w*","",tweet)#remove mentions
    tweet = re.sub("\n","",tweet)#remove lines separate
    tweet = unicodedata.normalize('NFKD', tweet).encode('ASCII', 'ignore')#remove accents
    tweet = re.sub('[^a-zA-Z ]+', ' ', tweet.decode('ASCII'))#remove punctuactions
    text_no_stop_words = filter_stop_words(tweet)
    tokens = [token for token in text_no_stop_words if len(token) > 2 ]
    tweet = " ".join(tokens)
    return tweet

file_worker =  FileStorage("E:\Programs Files\JetBrains\PyCharm\PycharmProjects\DataAnalytics-using-MachineLearning\Data Extraction\Twitter\TIMELINE.json")
data = file_worker.read()
corpus = []
tweets_collection = {}
id = 0
for tweet in data["tweets"][0]:
    text = tweet["full_text"]
    if re.search("RT",text) == None:
        text_filtered = process(text)
        line = "%s\n"%(text_filtered)
        corpus.append(line)
        tweets_collection[id] = { "text": text }
        id += 1

tf_idf_model = TfIdfModel(corpus)
tf_idf_model.fit()

n_top = 20
query_text = "PS5 PlayStation Play Station"

vector_query = tf_idf_model.project(query_text)
nearest_neighbors = tf_idf_model.get_nearest_neighbors(vector_query,n_top,cosine_similarity)
for id,score in nearest_neighbors:
    print ( tweets_collection[id]["text"],score)

filename = "./data/eluniversocom.json"
file_worker = FileStorage(filename)
data = file_worker.read()
posts = data["GraphImages"]
corpus = []
tweets_collection = {}
id = 0
for post in posts:
    comments = post["comments"]["data"]
    for comment in comments:
        corpus.append(comment["text"])
        tweets_collection[id] = { "text": comment["text"] }
        id += 1

tf_idf_model = TfIdfModel(corpus)
tf_idf_model.fit()

n_top = 20
query_text = "guillermo lasso"

vector_query = tf_idf_model.project(query_text)
nearest_neighbors = tf_idf_model.get_nearest_neighbors(vector_query,n_top,cosine_similarity)
for id,score in nearest_neighbors:
    print ( tweets_collection[id]["text"],score)

