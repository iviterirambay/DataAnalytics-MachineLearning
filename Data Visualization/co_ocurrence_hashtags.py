# -*- coding: utf-8 -*-
"""Co Ocurrence Hashtags.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N8vk4drdH_BB2ThMmSf1fY_PZEnvTRVh
"""

import os
import json
import itertools

class FileStorage():

    def __init__(self,filename):
        self.filename = filename

    def read(self):
        if os.path.exists(self.filename):
            with open(self.filename) as file:
                data = json.load(file)
                return data
        else:
            return {}
        
    def save(self,data):#write data like json file
        try:
            old_data = self.read()
            if len(old_data.keys()) == 0:
                old_data["tweets"] = []
            old_data["tweets"].append(data)
            jsondata = json.dumps(old_data, indent=4, skipkeys=True, sort_keys=True)
            fd = open(self.filename, 'w')
            fd.write(jsondata)
            fd.close()
            print (self.filename + " ha sido escrito exitosamente")
        except Exception as e:
            print (e)
            print ('ERROR writing', self.filename)

#remove stop words
import nltk
from nltk.corpus import stopwords
import re
import sys
import unicodedata


def filter_stop_words(text):
    stop_words_list = stopwords.words('spanish')
    stop_words_list += stopwords.words('english')
    text_filtered = [word for word in text.split() if word.lower() not in stop_words_list]
    return text_filtered

def process(tweet):
    tweet = tweet.lower()
    tweet = re.sub(r"(https?)\S+","",tweet)#remove urls
    #tweet = re.sub(r"(\B#)\w*","",tweet)#remove hashtags
    tweet = re.sub(r"(\B@)\w*","",tweet)#remove mentions
    tweet = re.sub("\n","",tweet)#remove lines separate
    tweet = unicodedata.normalize('NFKD', tweet).encode('ASCII', 'ignore')#remove accents
    tweet = re.sub('[^a-zA-Z ]+', ' ', tweet.decode('ASCII'))#remove punctuactions
    text_no_stop_words = filter_stop_words(tweet)
    tokens = [token for token in text_no_stop_words if len(token) > 2 ]
    tweet = " ".join(tokens)
    return tweet

file_worker = FileStorage("buenos_aires.json")
data = file_worker.read()
print(data['tweets'][0])

hashtags_count = {}

#for tweet in data['tweets'][8:]:
for tweet in data['tweets']:
#    print(tweet['entities']['hashtags'])
    tweet_hashtags = tweet['entities']['hashtags']
    hashtags_list = [hashtag['text'].lower() for hashtag in tweet_hashtags]
    hashtags_list.sort()
#    print(hashtags_list)
    hashtags_comb = list(itertools.combinations(hashtags_list, 2))
#    print(hashtags_comb)
    for comb in hashtags_comb:
        if comb not in hashtags_count:
            hashtags_count[comb] = 1
        else:
            hashtags_count[comb] += 1
#    print(hashtags_count)
#    break

hashtags_count

f = open("hashtags_network.csv", "w")
f.write('source,target,weight\n')
for comb in hashtags_count:
    count = str(hashtags_count[comb])
    hashtag1 = comb[0]
    hashtag2 = comb[1]
    print(hashtag1, hashtag2, count)
    f.write(hashtag1 + ',' + hashtag2 + ',' + count + '\n')
f.close()





